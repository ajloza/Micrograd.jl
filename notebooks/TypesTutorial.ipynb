{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porting Micrograd to Julia, Part 1: Building a backpropagation engine\n",
    "\n",
    "This is a port of Andrej Karpapthy's excellent package `Micrograd` to Julia. See his comprehensive walkthrough for details. A bare-bones exposition is included here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the right next step\n",
    "Fitting models requires a mechanism to determine how to adjust the parameters of the model in order to minimize the difference between the predicted and actual values (loss) or this loss plus a penalty based on model complexity (regularization term).\n",
    "\n",
    "Autodifferentiation provides a way to find the right way to adjust the parameters. Here we look at *reverse mode* or *backpropagation*.\n",
    "\n",
    "#### Local derivatives\n",
    "let's take a look at a simple addition of two values:\n",
    "```\n",
    "a = 1\n",
    "b = 2\n",
    "c = a + b\n",
    "```\n",
    "\n",
    "Derivatives tell us how changes in a or b impact c:\n",
    "- `dc/da` tells us how `c` will change with `a` change in `a`\n",
    "- `dc/db` tells us how `c` will change with `a` change in `b`\n",
    "\n",
    "Here:\n",
    "```\n",
    "dc/da = 1 + 0 = 1\n",
    "dc/da = 0 + 1 = 1\n",
    "```\n",
    "\n",
    "what about `f`? how do changes in `d` and `e` change the value of `f`?\n",
    "```\n",
    "d = 1\n",
    "e = 2\n",
    "f = d*e\n",
    "\n",
    "df/dd = e = 2 # in this instance because we know the value of e\n",
    "df/de = d = 1 # plugging in\n",
    "```\n",
    "\n",
    "#### propagating derivatives\n",
    "\n",
    "Models are not just one layer of calucations deep. we need a way to link changes in the output across layers of calculation\n",
    "\n",
    "Now let's look at `g`:\n",
    "```\n",
    "g = c + f\n",
    "```\n",
    "\n",
    "How do changes in `a` affect `g`? We can't see this directly, but the chain rule tells us how derivates combine.\n",
    "\n",
    "```\n",
    "dg/da = dg/dc * dc/da\n",
    "        -----   -----\n",
    "        ^       ^ a's local derivative wrt c\n",
    "        c's local derivative wrt g\n",
    "```\n",
    "\n",
    "This could be arbitrarily deep but each step can be decomposed into the passed derivative and local derivative.\n",
    "\n",
    "```\n",
    "loss = f(g(param,X),y) # where f is the loss function and g is the model\n",
    "\n",
    "d_loss/d_param    =   d_loss/d_1 * d_1/d_2 * d_i/d_i+1 * d_n/d_param\n",
    "--------------        --------------------------------   -----------\n",
    "^ goal derivative     ^ passed derivative                ^ local derivative                     \n",
    "```\n",
    "\n",
    "\n",
    "So we need the following things to acheive backpropagation and adjust the parameters to monimize the loss:\n",
    "1. a record of the computation chain from the parameter to the loss\n",
    "2. the local derivative of each computation\n",
    "3. a way to combine the local derivative with the passed derivative \n",
    "\n",
    "This package implements these."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "\n",
      "backward function\n",
      "1.0 (gr: 0.0, op:  )\n"
     ]
    }
   ],
   "source": [
    "using Micrograd\n",
    "# the core type in this implementation is `Value`. It is a numerical value with\n",
    "# associated attributes that are required to build a computation graph\n",
    "a = value(1.0)\n",
    "\n",
    "# it has the following fields. we'll see what they mean\n",
    "println(a.data) # the numeric value\n",
    "println(a.grad) # the gradient wrt the target, initially zero\n",
    "println(a.prev) # children values this one was contructed from, here none since it's a leaf, will need this to build the chain\n",
    "println(a.op) # operation used in the construction of this value (functions as a label), nothing here\n",
    "println(a.bw) # backward function, the thing that determines how to combine the passed derivative with the local derivative\n",
    "\n",
    "# the value prints out in abbreviated from\n",
    "println(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value c, it's gradient and the operation to create it: \n",
      "\t c = 3.0 (gr: 0.0, op: +)\n",
      "c is linked to a and b:\n",
      "\t c.prev = (1.0 (gr: 0.0, op:  ), 2.0 (gr: 0.0, op:  ))\n"
     ]
    }
   ],
   "source": [
    "# Let's look at a simple computation.\n",
    "a = value(1.0)\n",
    "b = value(2.0)\n",
    "c = a+b\n",
    "println(\"The value c, it's gradient and the operation to create it: \\n\\t c = $c\")\n",
    "println(\"c is linked to a and b:\\n\\t c.prev = $(c.prev)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inside an operation\n",
    "The plus operation looks like this:\n",
    "\n",
    "```julia\n",
    "function Base.:+(x::Value{T},y::Value{T}) where T \n",
    "    out = value(x.data+y.data,zero(T),(x,y),\"+\")\n",
    "    function bwf()\n",
    "        x.grad += out.grad  # dL/dx = dout/dx * dL/dout = 1.0 (local) * out.grad (passed) = out.grad\n",
    "        y.grad += out.grad\n",
    "        return nothing\n",
    "    end\n",
    "    out.bw = Backward(bwf)\n",
    "    return out\n",
    "end\n",
    "```\n",
    "\n",
    "Backpropagation works from parents to children. At the construction of each parent, a specific function is created to combines the local derivative from the operation with the passed derivative from the parent wrt the target to find the gradient of the children wrt the target.\n",
    "\n",
    "Here's what `*` looks like:\n",
    "```julia\n",
    "function Base.:*(x::Value{T},y::Value{T}) where T \n",
    "    out = value(x.data*y.data,zero(T),(x,y),\"*\")\n",
    "    function bwf()\n",
    "        x.grad += y.data*out.grad\n",
    "        y.grad += x.data*out.grad\n",
    "    end\n",
    "    out.bw = Backward(bwf)\n",
    "    return out\n",
    "end\n",
    "```\n",
    "\n",
    "This is exactly what we saw above. The local derivative for one child is the value of the other.\n",
    "\n",
    "```julia\n",
    "d = 1\n",
    "e = 2\n",
    "f = d*e\n",
    "\n",
    "df/dd = e = 2 # in this instance because we know the value of e\n",
    "df/de = d = 1 # plugging in\n",
    "```\n",
    "\n",
    "You can work out by hand local derivatives for any function and write the backward function.\n",
    "\n",
    "```julia\n",
    "function relu(x::Value{T}) where T\n",
    "    out_data = x.data > 0.0 ? x.data : 0.0\n",
    "    out = value(out_data,zero(T),(x,),\"relu\")\n",
    "    function bwf()\n",
    "        x.grad += x.data > 0.0 ? out.grad : 0.0   # dL/dx = dout/dx (1.0 or 0.0) *  dL/dout (out.grad)\n",
    "    end\n",
    "    out.bw = Backward(bwf)\n",
    "    return out\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple Polytree Viewer: \n",
      "\n",
      "1.0 (gr: 0.0, op: tanh)   \n",
      "|\n",
      "3.0 (gr: 0.0, op: +)   \n",
      "|----------------------|\n",
      "2.0 (gr: 0.0, op:  )   1.0 (gr: 0.0, op:  )   "
     ]
    }
   ],
   "source": [
    "# we can add a bit more and show the new computation graph\n",
    "d = tanh(c)\n",
    "nodes,depth = buildgraph(d)\n",
    "printgraph(nodes,depth) # see doc string for what this function can and can't do\n",
    "\n",
    "# note that all gradients are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple Polytree Viewer: \n",
      "\n",
      "1.0 (gr: 1.0, op: tanh)   \n",
      "|\n",
      "3.0 (gr: 0.0099, op: +)   \n",
      "|----------------------|\n",
      "2.0 (gr: 0.0, op:  )   1.0 (gr: 0.0, op:  )   "
     ]
    }
   ],
   "source": [
    "# we can set the gradient of d and then call it's backward function\n",
    "d.grad = 1.0\n",
    "d.bw()\n",
    "printgraph(nodes,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple Polytree Viewer: \n",
      "\n",
      "1.0 (gr: 1.0, op: tanh)   \n",
      "|\n",
      "3.0 (gr: 0.0099, op: +)   \n",
      "|-------------------------|\n",
      "2.0 (gr: 0.0099, op:  )   1.0 (gr: 0.0099, op:  )   "
     ]
    }
   ],
   "source": [
    "# this propagated the derivative one step back. now call c's backward function\n",
    "c.bw()\n",
    "printgraph(nodes,depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Value[1.0 (gr: 0.0099, op:  ), 2.0 (gr: 0.0099, op:  ), 3.0 (gr: 0.0099, op: +), 1.0 (gr: 1.0, op: tanh)], [3, 3, 2, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can build the computational graph using topological sort such that a node is only added if it's children have already been added\n",
    "g = buildgraph(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple Polytree Viewer: \n",
      "\n",
      "1.0 (gr: 1.0, op: tanh)   \n",
      "|\n",
      "3.0 (gr: 0.0099, op: +)   \n",
      "|-------------------------|\n",
      "2.0 (gr: 0.0099, op:  )   1.0 (gr: 0.0099, op:  )   "
     ]
    }
   ],
   "source": [
    "# now if we go backward through this list, we will only be passing back gradients that are fully calculated\n",
    "zerograd(d) # gradients acumulate, need to reset them.\n",
    "d.grad = 1.0\n",
    "for n in reverse(nodes)\n",
    "    n.bw()\n",
    "end\n",
    "printgraph(nodes,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 0.9950547536867305\n",
      "d2: 0.9966646024355773\n"
     ]
    }
   ],
   "source": [
    "# now if we want to make d larger we know what to do\n",
    "a2 = a+a.grad*10 # unrealistically massive step size, but we're in the tail of tanh\n",
    "b2 = b+b.grad*10\n",
    "c2 = a2+b2\n",
    "d2 = tanh(c2)\n",
    "\n",
    "println(\"d: $(d.data)\")\n",
    "println(\"d2: $(d2.data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 0.9950547536867305\n",
      "d2: 0.9926707543920653\n"
     ]
    }
   ],
   "source": [
    "# or smaller\n",
    "a2 = a-a.grad*10 # unrealistically massive step size, but we're in the tail of tanh\n",
    "b2 = b-b.grad*10\n",
    "c2 = a2+b2\n",
    "d2 = tanh(c2)\n",
    "\n",
    "println(\"d: $(d.data)\")\n",
    "println(\"d2: $(d2.data)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "couple this with a gradient descent algorithm and you have a solver!\n",
    "\n",
    "### A single neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre backpropagation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tree:\n",
      "----- \n",
      "\n",
      "0.71 (gr: 0.0, op: tanh)   \n",
      "|\n",
      "0.88 (gr: 0.0, op: +)   \n",
      "|----------------------|\n",
      "6.9 (gr: 0.0, op:  )   -6.0 (gr: 0.0, op: +)   \n",
      "                       |---------------------------------------------|\n",
      "                       0.0 (gr: 0.0, op: *)                          -6.0 (gr: 0.0, op: *)   \n",
      "                       |----------------------|                      |-----------------------|\n",
      "                       1.0 (gr: 0.0, op:  )   0.0 (gr: 0.0, op:  )   -3.0 (gr: 0.0, op:  )   2.0 (gr: 0.0, op:  )   \n",
      "\n",
      "Post backpropagation:\n",
      "\n",
      "Tree:\n",
      "----- \n",
      "\n",
      "0.71 (gr: 1.0, op: tanh)   \n",
      "|\n",
      "0.88 (gr: 0.5, op: +)   \n",
      "|----------------------|\n",
      "6.9 (gr: 0.5, op:  )   -6.0 (gr: 0.5, op: +)   \n",
      "                       |---------------------------------------------|\n",
      "                       0.0 (gr: 0.5, op: *)                          -6.0 (gr: 0.5, op: *)   \n",
      "                       |----------------------|                      |-----------------------|\n",
      "                       1.0 (gr: 0.0, op:  )   0.0 (gr: 0.5, op:  )   -3.0 (gr: 1.0, op:  )   2.0 (gr: -1.5, op:  )   "
     ]
    }
   ],
   "source": [
    "## single neuron example, replicating micrograd example\n",
    "x1 = value(2.0)\n",
    "w1 = value(-3.0)\n",
    "x2 = value(0.0)\n",
    "w2 = value(1.0)\n",
    "b = value(6.8813735870195432)\n",
    "\n",
    "x1w1 = x1*w1\n",
    "x2w2 = x2*w2\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2\n",
    "n = x1w1x2w2+b\n",
    "o = tanh(n)\n",
    "\n",
    "\n",
    "println(\"Pre backpropagation:\")\n",
    "nodes,depth = buildgraph(o)\n",
    "printgraph(nodes,depth)\n",
    "\n",
    "println(\"\\n\\nPost backpropagation:\")\n",
    "\n",
    "backward(o)\n",
    "printgraph(nodes,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
