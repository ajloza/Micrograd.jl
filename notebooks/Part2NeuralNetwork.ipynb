{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Neural Networks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons\n",
    "A neuron is just a container for weights, a bias, and an activation function. It takes a number of inputs, combines them, and produces a scalar output. The number of weights is determined by the number of inputs (features) to that neuron. The activation function allows for more complex mapping of inputs to the output (and so that layers of neurons don't reduce to a single linear combination). The code defining a neuron is here:\n",
    "\n",
    "```julia\n",
    "    mutable struct Neuron{T<:AbstractFloat,F<:Union{Function,Nothing}}\n",
    "        w::Vector{Value{T}}\n",
    "        b::Value{T}\n",
    "        activation::F\n",
    "    end\n",
    "```\n",
    "We can define a function to make variables of this type \"callable\": `n(X)`\n",
    "\n",
    "```julia\n",
    "    function (n::Neuron)(x)\n",
    "        if length(x) != length(n.w)\n",
    "            error(\"In calling n(x), expected $(length(n.w)) inputs to neuron, got $(length(x)):\\n\\tx = $(x)\")\n",
    "        end\n",
    "        raw = sum(n.w .* x)+ n.b\n",
    "        isnothing(n.activation) && return raw \n",
    "        return n.activation(raw)\n",
    "    end\n",
    "```\n",
    "This just computes the inner product of X and the weights, adds b, and then runs the result through an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.2 (gr: 0.0, op: +)\n"
     ]
    }
   ],
   "source": [
    "using Micrograd\n",
    "\n",
    "## make a linear neuron with three inputs and call it\n",
    "n = neuron(3,nothing)\n",
    "x = [1.0,2.0,1.0]\n",
    "o = n(x)\n",
    "println(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tree:\n",
      "----- \n",
      "\n",
      "-1.2 (gr: 0.0, op: +)   \n",
      "|------------------------|\n",
      "-0.84 (gr: 0.0, op:  )   -0.39 (gr: 0.0, op: +)   \n",
      "                         |----------------------------------------------|\n",
      "                         0.02 (gr: 0.0, op: *)                          -0.41 (gr: 0.0, op: +)   \n",
      "                         |----------------------|                       |-----------------------------------------------|\n",
      "                         1.0 (gr: 0.0, op:  )   0.02 (gr: 0.0, op:  )   -0.98 (gr: 0.0, op: *)                          0.58 (gr: 0.0, op: *)   \n",
      "                                                                        |----------------------|                        |----------------------|\n",
      "                                                                        2.0 (gr: 0.0, op:  )   -0.49 (gr: 0.0, op:  )   1.0 (gr: 0.0, op:  )   0.58 (gr: 0.0, op:  )   "
     ]
    }
   ],
   "source": [
    "# now let's look at the tree\n",
    "nodes,depth = buildgraph(o)\n",
    "printgraph(nodes,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ErrorException(\"In calling n(x), expected 3 inputs to neuron, got 2:\\n\\tx = [1.0, 2.0]\")\n",
      "\n",
      " ^ correctly throws error\n"
     ]
    }
   ],
   "source": [
    "# you can't give the wrong number of inputs\n",
    "try\n",
    "    n([1.0,2.0])\n",
    "catch e\n",
    "    println(e)\n",
    "    println(\"\\n ^ correctly throws error\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tree:\n",
      "----- \n",
      "\n",
      "1.0 (gr: 0.0, op: relu)   \n",
      "|\n",
      "1.0 (gr: 0.0, op: +)   \n",
      "|----------------------|\n",
      "0.9 (gr: 0.0, op:  )   0.11 (gr: 0.0, op: +)   \n",
      "                       |------------------------------------------------|\n",
      "                       -0.028 (gr: 0.0, op: *)                          0.14 (gr: 0.0, op: +)   \n",
      "                       |----------------------|                         |-----------------------------------------------|\n",
      "                       1.0 (gr: 0.0, op:  )   -0.028 (gr: 0.0, op:  )   -0.43 (gr: 0.0, op: *)                          0.57 (gr: 0.0, op: *)   \n",
      "                                                                        |----------------------|                        |----------------------|\n",
      "                                                                        2.0 (gr: 0.0, op:  )   -0.22 (gr: 0.0, op:  )   1.0 (gr: 0.0, op:  )   0.57 (gr: 0.0, op:  )   "
     ]
    }
   ],
   "source": [
    "## make a relu activation neuron with three inputs and call it\n",
    "# note the op here is now relu instead of +\n",
    "# run this multiple times to see relu effect on a negative value\n",
    "n = neuron(3,relu)\n",
    "x = [1.0,2.0,1.0]\n",
    "o = n(x) \n",
    "\n",
    "nodes,depth = buildgraph(o)\n",
    "printgraph(nodes,depth)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "Layers are just groups of neurons, all with the same inputs (or at least here with fully connected layers). They essentially map from N dimensional input to M dimensional outputs where N is the number of inputs to each neuron in the layer and M is the number of neurons in the layer (recall each neuron has one output).\n",
    "\n",
    "The code is here:\n",
    "```julia\n",
    "mutable struct Layer{T<:AbstractFloat,F<:Union{Function,Nothing}}\n",
    "    neurons::Vector{Neuron{T,F}}\n",
    "    inputs::Int\n",
    "    outputs::Int\n",
    "end\n",
    "```\n",
    "\n",
    "and again we make the layer callable:\n",
    "\n",
    "```julia\n",
    "function (l::Layer)(x)\n",
    "    out = [n(x) for n in l.neurons]\n",
    "    length(out) == 1 && return out[1]\n",
    "    return out\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tree:\n",
      "----- \n",
      "\n",
      "2.8 (gr: 0.0, op: relu)   \n",
      "|\n",
      "2.8 (gr: 0.0, op: +)   \n",
      "|-----------------------|\n",
      "0.91 (gr: 0.0, op:  )   1.9 (gr: 0.0, op: +)   \n",
      "                        |----------------------------------------------|\n",
      "                        0.64 (gr: 0.0, op: *)                          1.3 (gr: 0.0, op: +)   \n",
      "                        |----------------------|                       |----------------------------------------------|\n",
      "                        1.0 (gr: 0.0, op:  )   0.64 (gr: 0.0, op:  )   1.3 (gr: 0.0, op: *)                           -0.074 (gr: 0.0, op: *)   \n",
      "                                                                       |----------------------|                       |----------------------|\n",
      "                                                                       2.0 (gr: 0.0, op:  )   0.67 (gr: 0.0, op:  )   1.0 (gr: 0.0, op:  )   -0.074 (gr: 0.0, op:  )   "
     ]
    }
   ],
   "source": [
    "## make a layer of a single neuron with relu activation\n",
    "# this is just a layer wrapper for one neuron to make sure the API works and not break the printing\n",
    "l = layer(3,1,relu)\n",
    "o = l(x)\n",
    "\n",
    "nodes,depth = buildgraph(o)\n",
    "printgraph(nodes,depth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptrons\n",
    "\n",
    "These are simply lists of layers where the output of one is the input to the next. This means the n_in and n_out have to line up in the series. \n",
    "\n",
    "Here's the simple definition of the MLP type:\n",
    "```julia\n",
    "mutable struct MLP\n",
    "    layers::Vector{Layer}\n",
    "end\n",
    "```\n",
    "\n",
    "The constructor takes how many inputs (from the features in the data set) and a list of how many neurons per layer and makes sure the ins and outs line up.\n",
    "\n",
    "```julia\n",
    "function mlp(n_in,n_outs,act=relu)\n",
    "    n_vec = [n_in;n_outs]\n",
    "    MLP([i!= length(n_outs) ? layer(n_vec[i],n_vec[i+1],act) : layer(n_vec[i],n_vec[i+1],nothing) for i in 1:length(n_outs)])\n",
    "end\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "At the end, we need to make sure the output maps to the correct domain for our problem. That's why the last one has a linear output. We don't want to truncate the domain with an activation function by default.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a NN\n",
    "\n",
    "Instead of showing a simple MLP which is impossible to print with the tools in this package, we'll define the same one used in Micrograd to classify the sklearn moons data.\n",
    "\n",
    "This is an MLP that takes in 2D points and predicts a binary class. It uses 3 layers of 16 (relu), 16 (relu), and 1 (linear). The ending linear layer makes sure we can actually calculate hinge loss. Try setting it to relu and notice it won't fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\tLoss: 2.5\tAccuracy: 0.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1\tLoss: 4.3\tAccuracy: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2\tLoss: 1.1\tAccuracy: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3\tLoss: 1.3\tAccuracy: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4\tLoss: 0.88\tAccuracy: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5\tLoss: 0.59\tAccuracy: 0.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6\tLoss: 0.41\tAccuracy: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7\tLoss: 0.27\tAccuracy: 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8\tLoss: 0.24\tAccuracy: 0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9\tLoss: 0.25\tAccuracy: 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10\tLoss: 0.25\tAccuracy: 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 11\tLoss: 0.31\tAccuracy: 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 12\tLoss: 0.21\tAccuracy: 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 13\tLoss: 0.23\tAccuracy: 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 14\tLoss: 0.17\tAccuracy: 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 15\tLoss: 0.17\tAccuracy: 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 16\tLoss: 0.16\tAccuracy: 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 17\tLoss: 0.2\tAccuracy: 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 18\tLoss: 0.12\tAccuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 19\tLoss: 0.13\tAccuracy: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 20\tLoss: 0.13\tAccuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 21\tLoss: 0.19\tAccuracy: 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 22\tLoss: 0.2\tAccuracy: 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 23\tLoss: 0.17\tAccuracy: 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 24\tLoss: 0.1\tAccuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 25\tLoss: 0.097\tAccuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 26\tLoss: 0.14\tAccuracy: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 27\tLoss: 0.21\tAccuracy: 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 28\tLoss: 0.13\tAccuracy: 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 29\tLoss: 0.072\tAccuracy: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 30\tLoss: 0.071\tAccuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 31\tLoss: 0.11\tAccuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 32\tLoss: 0.18\tAccuracy: 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 33\tLoss: 0.16\tAccuracy: 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 34\tLoss: 0.058\tAccuracy: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 35\tLoss: 0.052\tAccuracy: 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 36\tLoss: 0.074\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 37\tLoss: 0.083\tAccuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 38\tLoss: 0.044\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 39\tLoss: 0.046\tAccuracy: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 40\tLoss: 0.056\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 41\tLoss: 0.11\tAccuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 42\tLoss: 0.053\tAccuracy: 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 43\tLoss: 0.047\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 44\tLoss: 0.069\tAccuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 45\tLoss: 0.03\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 46\tLoss: 0.033\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 47\tLoss: 0.042\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 48\tLoss: 0.072\tAccuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 49\tLoss: 0.029\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 50\tLoss: 0.033\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 51\tLoss: 0.055\tAccuracy: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 52\tLoss: 0.023\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 53\tLoss: 0.024\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 54\tLoss: 0.035\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 55\tLoss: 0.065\tAccuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 56\tLoss: 0.026\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 57\tLoss: 0.034\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 58\tLoss: 0.059\tAccuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 59\tLoss: 0.025\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 60\tLoss: 0.029\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 61\tLoss: 0.053\tAccuracy: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 62\tLoss: 0.027\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 63\tLoss: 0.021\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 64\tLoss: 0.022\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 65\tLoss: 0.019\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 66\tLoss: 0.021\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 67\tLoss: 0.019\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 68\tLoss: 0.02\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 69\tLoss: 0.021\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 70\tLoss: 0.019\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 71\tLoss: 0.02\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 72\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 73\tLoss: 0.019\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 74\tLoss: 0.019\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 75\tLoss: 0.019\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 76\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 77\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 78\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 79\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 80\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 81\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 82\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 83\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 84\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 85\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 86\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 87\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 88\tLoss: 0.018\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 89\tLoss: 0.017\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 90\tLoss: 0.017\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 91\tLoss: 0.017\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 92\tLoss: 0.017\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 93\tLoss: 0.017\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 94\tLoss: 0.017\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 95\tLoss: 0.017\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 96\tLoss: 0.017\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 97\tLoss: 0.017\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 98\tLoss: 0.017\tAccuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 99\tLoss: 0.017\tAccuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "X,y = getmoons()\n",
    "m = mlp(2, [16, 16, 1])\n",
    "\n",
    "\n",
    "fit(m,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×2 Matrix{Float64}:\n",
       " -1.0  -1.0\n",
       " -1.0  -1.0\n",
       "  1.0   1.0\n",
       " -1.0  -1.0\n",
       "  1.0   1.0\n",
       "  1.0   1.0\n",
       "  1.0   1.0\n",
       "  1.0   1.0\n",
       "  1.0   1.0\n",
       " -1.0  -1.0\n",
       "  ⋮    \n",
       "  1.0   1.0\n",
       "  1.0   1.0\n",
       "  1.0   1.0\n",
       "  1.0   1.0\n",
       "  1.0   1.0\n",
       " -1.0  -1.0\n",
       "  1.0   1.0\n",
       "  1.0   1.0\n",
       " -1.0  -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ok there's no plotting library. this is not quite as satisfying.\n",
    "y_fit = m.(X)\n",
    "cat(y,sign.(getfield.(y_fit,:data)),dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \n",
      "-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \n",
      "-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \n",
      "-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \n",
      "-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \n",
      "-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \n",
      "-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \n",
      "-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \n",
      "-  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  +  \n",
      "-  -  -  -  -  -  -  -  -  -  -  +  +  +  +  -  -  -  -  -  -  -  -  +  +  \n",
      "-  -  -  -  -  -  -  -  -  -  +  +  +  +  +  +  -  -  -  -  -  -  +  +  +  \n",
      "-  -  -  -  -  -  -  -  -  +  +  +  +  +  +  +  +  -  -  -  -  +  +  +  +  \n",
      "-  -  -  -  -  -  -  -  -  +  +  +  +  +  +  +  +  +  -  -  -  +  +  +  +  \n",
      "-  -  -  -  -  -  -  -  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  \n",
      "-  -  -  -  -  -  -  -  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  \n",
      "-  -  -  -  -  -  -  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  \n",
      "-  -  -  -  -  -  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  \n",
      "-  -  -  -  -  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  \n",
      "-  -  -  -  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  \n",
      "-  -  -  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  \n",
      "-  -  -  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  \n",
      "-  -  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  \n",
      "-  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  \n",
      "-  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  \n",
      "+  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  "
     ]
    }
   ],
   "source": [
    "# but maybe the decision boundary is cooler\n",
    "xg = collect(range(-2.0,stop=2.0,length=25))\n",
    "yg = collect(range(2.0,stop=-2.0,length=25))\n",
    "\n",
    "yi_old = 0\n",
    "str = \"\"\n",
    "for (yi,y) in enumerate(collect(yg))\n",
    "    for (xi,x) in enumerate(collect(xg))\n",
    "        if yi!=yi_old\n",
    "            str = str*\"\\n\"\n",
    "        end\n",
    "        o = m([x,y])\n",
    "        if sign(o.data) == 1\n",
    "            str=str*\"+  \"\n",
    "        else\n",
    "            str=str*\"-  \"\n",
    "        end\n",
    "        yi_old = yi\n",
    "    end\n",
    "end\n",
    "print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[31m-  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\n",
      "\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m\u001b[34m+  \u001b[39m"
     ]
    }
   ],
   "source": [
    "# let's give it some color!\n",
    "xg = collect(range(-2.0,stop=2.0,length=25))\n",
    "yg = collect(range(2.0,stop=-2.0,length=25))\n",
    "\n",
    "yi_old = 0\n",
    "str = \"\"\n",
    "io = IOBuffer();\n",
    "for (yi,y) in enumerate(collect(yg))\n",
    "    for (xi,x) in enumerate(collect(xg))\n",
    "        if yi!=yi_old\n",
    "            print(\"\\n\")\n",
    "        end\n",
    "        o = m([x,y])\n",
    "        if sign(o.data) == 1\n",
    "            printstyled(\"+  \",color=:blue)\n",
    "        else\n",
    "            printstyled(\"-  \",color=:red)\n",
    "        end\n",
    "        yi_old = yi\n",
    "    end\n",
    "end\n",
    "print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
